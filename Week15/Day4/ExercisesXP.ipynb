{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd784732",
   "metadata": {},
   "source": [
    "## Exercise 1 - Calculating Required Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405ddfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group = 2940 users\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower \n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Inputs\n",
    "p1 = 0.20\n",
    "p2 = 0.23\n",
    "alpha = 0.05\n",
    "power = 0.8 \n",
    "\n",
    "# Calculate effect size (Cohen's h for proportions)\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate sample size per group \n",
    "analysis = NormalIndPower()\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Required sample size per group = {int(sample_size)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8a064",
   "metadata": {},
   "source": [
    "## Exercise 2 - Understanding The Relationship Between Effect Size And Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size - 0.2\n",
      "Required sample size per group = 6507 users\n",
      "\n",
      "Effect Size - 0.4\n",
      "Required sample size per group = 1680 users\n",
      "\n",
      "Effect Size - 0.5\n",
      "Required sample size per group = 1091 users\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower \n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Inputs\n",
    "alpha = 0.05\n",
    "power = 0.8 \n",
    "\n",
    "def sample_size(p1, p2, alpha, power)->str:\n",
    "    \"\"\"Calculates sample size needed for AB testing for discrete metrics\n",
    "      based on p1, p2, alpha and power\"\"\"\n",
    "\n",
    "    # Calculate effect size (Cohen's h for proportions)\n",
    "    effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "    # Calculate sample size per group\n",
    "    analysis = NormalIndPower()\n",
    "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "    print(f\"Required sample size per group = {int(sample_size)} users\\n\")\n",
    "\n",
    "# Calculate sample size for different effect sizes \n",
    "\n",
    "# +%2\n",
    "print(\"Effect Size - 0.2\")\n",
    "sample_size(0.20, 0.22, alpha, power)\n",
    "\n",
    "# +%4\n",
    "print(\"Effect Size - 0.4\")\n",
    "sample_size(0.20, 0.24, alpha, power)\n",
    "\n",
    "# +5%\n",
    "print(\"Effect Size - 0.5\")\n",
    "sample_size(0.20, 0.25, alpha, power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839fbaf",
   "metadata": {},
   "source": [
    "**How does the sample size change as the effect size increases? Explain why this happens.**\n",
    "\n",
    "\n",
    "Answer: If we want to detect a smaller effect size, we’ll need more users to be able to identify a small change in our metric. Conversely, if the expected effect size is larger, the required sample size will be smaller, since it’s easier to detect a bigger change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a1d92",
   "metadata": {},
   "source": [
    "## Exercise 3 - Exploring The Impact Of Statistical Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d9e9f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power = 0.7\n",
      "Required sample size per group = 5117 users\n",
      "\n",
      "Power = 0.8\n",
      "Required sample size per group = 6507 users\n",
      "\n",
      "Power = 0.9\n",
      "Required sample size per group = 8711 users\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower \n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Inputs\n",
    "alpha = 0.05\n",
    "p1 = 0.20\n",
    "p2 = 0.22 \n",
    "\n",
    "def sample_size(p1, p2, alpha, power):\n",
    "    \"\"\"Calculates sample size needed for AB testing for discrete metrics\n",
    "    based on p1, p2, alpha and power\"\"\"\n",
    "     \n",
    "    # Calculate effect size\n",
    "    effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "    # Calculate sample size per group\n",
    "    analysis = NormalIndPower()\n",
    "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "    print(f\"Required sample size per group = {int(sample_size)} users\\n\")\n",
    "\n",
    "# Calculate required sample size for different power levels \n",
    "# 0.7\n",
    "print(\"Power = 0.7\")\n",
    "sample_size(p1, p2, alpha, 0.7)\n",
    "# 0.8\n",
    "print(\"Power = 0.8\")\n",
    "sample_size(p1, p2, alpha, 0.8)\n",
    "# 0.9\n",
    "print(\"Power = 0.9\")\n",
    "sample_size(p1, p2, alpha, 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a609e4f",
   "metadata": {},
   "source": [
    "**Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?**\n",
    "\n",
    "If we increase the statistical power, our sample size will increase as well. \n",
    "It's important to understand this because if we don't have an enough users to run tests, we'll need to compromise on one of the components of power analysis. If we compromise on power, we need to take into consideration that the lower the statistical power the lower the chance that we'll catch a difference between control and variant if there is a difference. \n",
    "So, if we lower the power to 0.7 we need less users, but there's a 30% chance that we'll run the test and see no significant difference and wrongly conclude that variant version isn't better - even though it actually is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc1c65",
   "metadata": {},
   "source": [
    "## Exercise 4 - Implementing Sequential Testing\n",
    "\n",
    "You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
    "\n",
    "- Define your stopping criteria.\n",
    "- Decide how you would implement sequential testing in this scenario.\n",
    "- At the end of week three, Version B has a p-value of 0.02. What would you do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437d4e0",
   "metadata": {},
   "source": [
    "Answer: \n",
    "- Stopping criteria: One version has a p-value less than 0.05 at any checkpoint. \n",
    "- I'd monitor the test every week, for example:\n",
    "\n",
    "-- In week 1 I'd check the conversion rate for Version A and Version B, if I see the p-value is above 0.05 (stopping criteria) or that both versions have the same value, I continue monitoring. If the p-value is below 0.05 I conclude that Version B is better and stop the test. \n",
    "\n",
    "-- I repeat the same step on week 2. Continuing to monitor if the p-value is above 0.05 or stop the test if I see the p-value is below 0.05. \n",
    "\n",
    "-- In week 3 (assuming this is the last week of the test), I check the results, if Version B has a significantly higher conversion rate than version A and the analysis gives me a p-value < 0.05 I stop the test and conclude that Version B is better, else, I stop the test anyway and I let the stakeholders know that Version B didn't perform better than Version A and we should stay with the same version or continue with a different test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8c1d5",
   "metadata": {},
   "source": [
    "## Exercise 5 - Applying Bayesian A/B Testing\n",
    "\n",
    "You’re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
    "\n",
    "- Describe how you would set up your prior belief.\n",
    "- After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
    "- What would you do if the posterior probability was only 55%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140db58",
   "metadata": {},
   "source": [
    "- I would set up my prior belief on previous tests that we ran on similar apps or market research about how similar features impacted user engagement on different apps from the same industry. \n",
    "- Even if it shows a 15% increase, I think it would be more beneficial to do more testing and come to a more conclusive result before proceding with either decision.\n",
    "- I'd advice to continue testing to get a more conclusive result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d55d5",
   "metadata": {},
   "source": [
    "## Exercise 6 - Implementing Adaptive Experimentation\n",
    "\n",
    "You’re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
    "\n",
    "- Explain how you would adjust the traffic allocation after the first week.\n",
    "- Describe how you would continue to adapt the experiment in the following weeks.\n",
    "- What challenges might you face with adaptive experimentation, and how would you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1914b6e",
   "metadata": {},
   "source": [
    "- After the first week I would put 40% of traffic on Layout C and the rest divide it half/half between Layout A and B\n",
    "- If in the following week I see that Layout C is still performing better than A and B I would adjust 50% of the traffic to the best performance layout (C) and leave the rest half half on A and B, if on following weeks it continues to overperform we can keep increassing the traffic to layout C and decide to implement based on what our stakeholders think.\n",
    "- There are a few challenges that can come with adaptive experimentation. There might be bias in estimates as we're not comparing apples to apples when we shift users from one variant to the other, it's harder to calculate confidence intervals or do proper statistical tests at the end as the sample sizes and allocations of the variants are unequal and changing. This can be addressed by:\n",
    "\n",
    "- Deciding in advance when and how you'll adapt. \n",
    "- Make sure to split traffic equally among variations at the beginning (including bad ones) so you learn slowly. \n",
    "- Keep a small percentage of traffic (5-10%) allocated equally to all variations throughout the test as it gives an unbiased estimate of true effects. \n",
    "- Use bayesian methods. \n",
    "- If the stakes of implementing the \"winner\" variant are high, a quick traditional A/B test should be run on the winner vs control with fresh data. This confirms findings with clean statistics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
